{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-10-03T14:53:28.876887Z",
     "iopub.status.busy": "2022-10-03T14:53:28.876471Z",
     "iopub.status.idle": "2022-10-03T14:53:30.230086Z",
     "shell.execute_reply": "2022-10-03T14:53:30.228131Z",
     "shell.execute_reply.started": "2022-10-03T14:53:28.876770Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'nvidia-smi' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-03T14:53:56.050989Z",
     "iopub.status.busy": "2022-10-03T14:53:56.048158Z",
     "iopub.status.idle": "2022-10-03T14:53:57.441739Z",
     "shell.execute_reply": "2022-10-03T14:53:57.439981Z",
     "shell.execute_reply.started": "2022-10-03T14:53:56.050953Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!wget -O prompts.csv https://raw.githubusercontent.com/krea-ai/open-prompts/main/data/1k.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-03T14:54:01.829027Z",
     "iopub.status.busy": "2022-10-03T14:54:01.828516Z",
     "iopub.status.idle": "2022-10-03T14:54:03.055341Z",
     "shell.execute_reply": "2022-10-03T14:54:03.053672Z",
     "shell.execute_reply.started": "2022-10-03T14:54:01.828981Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'ls' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-03T14:54:16.535580Z",
     "iopub.status.busy": "2022-10-03T14:54:16.534957Z",
     "iopub.status.idle": "2022-10-03T14:54:16.609028Z",
     "shell.execute_reply": "2022-10-03T14:54:16.607092Z",
     "shell.execute_reply.started": "2022-10-03T14:54:16.535534Z"
    }
   },
   "outputs": [],
   "source": [
    "prompts = pd.read_csv(\"prompts.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-03T14:55:16.215746Z",
     "iopub.status.busy": "2022-10-03T14:55:16.215335Z",
     "iopub.status.idle": "2022-10-03T14:55:16.233656Z",
     "shell.execute_reply": "2022-10-03T14:55:16.231981Z",
     "shell.execute_reply.started": "2022-10-03T14:55:16.215716Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>raw_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A portrait photo of a kangaroo wearing an oran...</td>\n",
       "      <td>{\"image_uri\": \"PENDING\", \"modifiers\": [\"portra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>inmates with cow heads inside a jailcell</td>\n",
       "      <td>{\"image_uri\": \"PENDING\", \"modifiers\": [\"inmate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>daguerrotype of a corgi astronaut on the moon,...</td>\n",
       "      <td>{\"image_uri\": \"PENDING\", \"modifiers\": [\"daguer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>totem animal tribal chaman vodoo mask feather ...</td>\n",
       "      <td>{\"image_uri\": \"PENDING\", \"modifiers\": [\"totem ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>p. cubensis</td>\n",
       "      <td>{\"image_uri\": \"PENDING\", \"modifiers\": [\"p cube...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>warrior tattoo, magnificent</td>\n",
       "      <td>{\"image_uri\": \"PENDING\", \"modifiers\": [\"warrio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>nematode worms find a plant cell, microscopy, ...</td>\n",
       "      <td>{\"image_uri\": \"PENDING\", \"modifiers\": [\"nemato...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>hyperrealism close-up portrait of beautiful me...</td>\n",
       "      <td>{\"image_uri\": \"PENDING\", \"modifiers\": [\"hyperr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>an extremely detailed masterpiece of a mouse w...</td>\n",
       "      <td>{\"image_uri\": \"PENDING\", \"modifiers\": [\"extrem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>a cinematic headshot portrait of a young woman...</td>\n",
       "      <td>{\"image_uri\": \"PENDING\", \"modifiers\": [\"cinema...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>998 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                prompt  \\\n",
       "0    A portrait photo of a kangaroo wearing an oran...   \n",
       "1             inmates with cow heads inside a jailcell   \n",
       "2    daguerrotype of a corgi astronaut on the moon,...   \n",
       "3    totem animal tribal chaman vodoo mask feather ...   \n",
       "4                                          p. cubensis   \n",
       "..                                                 ...   \n",
       "993                        warrior tattoo, magnificent   \n",
       "994  nematode worms find a plant cell, microscopy, ...   \n",
       "995  hyperrealism close-up portrait of beautiful me...   \n",
       "996  an extremely detailed masterpiece of a mouse w...   \n",
       "997  a cinematic headshot portrait of a young woman...   \n",
       "\n",
       "                                              raw_data  \n",
       "0    {\"image_uri\": \"PENDING\", \"modifiers\": [\"portra...  \n",
       "1    {\"image_uri\": \"PENDING\", \"modifiers\": [\"inmate...  \n",
       "2    {\"image_uri\": \"PENDING\", \"modifiers\": [\"daguer...  \n",
       "3    {\"image_uri\": \"PENDING\", \"modifiers\": [\"totem ...  \n",
       "4    {\"image_uri\": \"PENDING\", \"modifiers\": [\"p cube...  \n",
       "..                                                 ...  \n",
       "993  {\"image_uri\": \"PENDING\", \"modifiers\": [\"warrio...  \n",
       "994  {\"image_uri\": \"PENDING\", \"modifiers\": [\"nemato...  \n",
       "995  {\"image_uri\": \"PENDING\", \"modifiers\": [\"hyperr...  \n",
       "996  {\"image_uri\": \"PENDING\", \"modifiers\": [\"extrem...  \n",
       "997  {\"image_uri\": \"PENDING\", \"modifiers\": [\"cinema...  \n",
       "\n",
       "[998 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-03T15:00:24.534473Z",
     "iopub.status.busy": "2022-10-03T15:00:24.533904Z",
     "iopub.status.idle": "2022-10-03T15:00:24.547612Z",
     "shell.execute_reply": "2022-10-03T15:00:24.546217Z",
     "shell.execute_reply.started": "2022-10-03T15:00:24.534434Z"
    }
   },
   "outputs": [],
   "source": [
    "prompts_gt6 = prompts.loc[prompts.prompt.str.split(' ').str.len() > 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-03T15:01:56.300286Z",
     "iopub.status.busy": "2022-10-03T15:01:56.299834Z",
     "iopub.status.idle": "2022-10-03T15:02:16.625861Z",
     "shell.execute_reply": "2022-10-03T15:02:16.624213Z",
     "shell.execute_reply.started": "2022-10-03T15:01:56.300255Z"
    }
   },
   "outputs": [],
   "source": [
    "! pip install aitextgen -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-03T15:08:53.157959Z",
     "iopub.status.busy": "2022-10-03T15:08:53.157528Z",
     "iopub.status.idle": "2022-10-03T15:08:53.164512Z",
     "shell.execute_reply": "2022-10-03T15:08:53.162988Z",
     "shell.execute_reply.started": "2022-10-03T15:08:53.157901Z"
    }
   },
   "outputs": [],
   "source": [
    "model=\"EleutherAI/gpt-neo-125M\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-03T15:07:19.973622Z",
     "iopub.status.busy": "2022-10-03T15:07:19.973209Z",
     "iopub.status.idle": "2022-10-03T15:07:19.981938Z",
     "shell.execute_reply": "2022-10-03T15:07:19.980071Z",
     "shell.execute_reply.started": "2022-10-03T15:07:19.973577Z"
    }
   },
   "outputs": [],
   "source": [
    "prompts_gt6 = prompts_gt6.drop('raw_data', axis = 1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-03T15:07:41.443384Z",
     "iopub.status.busy": "2022-10-03T15:07:41.442957Z",
     "iopub.status.idle": "2022-10-03T15:07:41.460225Z",
     "shell.execute_reply": "2022-10-03T15:07:41.458768Z",
     "shell.execute_reply.started": "2022-10-03T15:07:41.443354Z"
    }
   },
   "outputs": [],
   "source": [
    "prompts_gt6.to_csv(\"input_text_cleaned.txt\", columns=[\"prompt\"], header=False, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-03T15:07:56.915746Z",
     "iopub.status.busy": "2022-10-03T15:07:56.915210Z",
     "iopub.status.idle": "2022-10-03T15:08:08.316368Z",
     "shell.execute_reply": "2022-10-03T15:08:08.314901Z",
     "shell.execute_reply.started": "2022-10-03T15:07:56.915702Z"
    }
   },
   "outputs": [],
   "source": [
    "from aitextgen.TokenDataset import TokenDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-03T15:08:10.465655Z",
     "iopub.status.busy": "2022-10-03T15:08:10.463010Z",
     "iopub.status.idle": "2022-10-03T15:08:10.882178Z",
     "shell.execute_reply": "2022-10-03T15:08:10.880864Z",
     "shell.execute_reply.started": "2022-10-03T15:08:10.465610Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de522687a8ac46549644075b1e167bbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/896 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = TokenDataset('./input_text_cleaned.txt', line_by_line=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-03T15:08:38.517099Z",
     "iopub.status.busy": "2022-10-03T15:08:38.515773Z",
     "iopub.status.idle": "2022-10-03T15:08:38.524060Z",
     "shell.execute_reply": "2022-10-03T15:08:38.522656Z",
     "shell.execute_reply.started": "2022-10-03T15:08:38.517033Z"
    }
   },
   "outputs": [],
   "source": [
    "from aitextgen import aitextgen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-03T15:26:53.063850Z",
     "iopub.status.busy": "2022-10-03T15:26:53.063403Z",
     "iopub.status.idle": "2022-10-03T15:27:28.235130Z",
     "shell.execute_reply": "2022-10-03T15:27:28.233456Z",
     "shell.execute_reply.started": "2022-10-03T15:26:53.063818Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /kaggle/working/aitextgen/tmpu8sup6me\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da925c525d0440689a1d0569a4e53350",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/502M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/pytorch_model.bin in cache at aitextgen/b0ace3b93ace62067a246888f1e54e2d3ec20807d4d3e27ac602eef3b7091c0b.6525df88f1d5a2d33d95ce2458ef6af9658fe7d1393d6707e0e318779ccc68ff\n",
      "creating metadata file for aitextgen/b0ace3b93ace62067a246888f1e54e2d3ec20807d4d3e27ac602eef3b7091c0b.6525df88f1d5a2d33d95ce2458ef6af9658fe7d1393d6707e0e318779ccc68ff\n",
      "https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /kaggle/working/aitextgen/tmp3us8kvj1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a209eb9e0f564ea98bc3e0e6bc4dc52b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/560 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/tokenizer_config.json in cache at aitextgen/3cc88b3aa29bb2546db2dc21783292e2a086bb7158c7b5ceddeb24158a85c183.e74f7c3643ee79eb023ead36008be72fe726dada60fa3b2a0569925cfefa1e74\n",
      "creating metadata file for aitextgen/3cc88b3aa29bb2546db2dc21783292e2a086bb7158c7b5ceddeb24158a85c183.e74f7c3643ee79eb023ead36008be72fe726dada60fa3b2a0569925cfefa1e74\n",
      "https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/vocab.json not found in cache or force_download set to True, downloading to /kaggle/working/aitextgen/tmpx87qu3ct\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68738eb9926b40ccb892116b96169e74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/vocab.json in cache at aitextgen/08c00c4159e921d4c941ac75732643373aba509d9b352a82bbbb043a94058d98.a552555fdda56a1c7c9a285bccfd44ac8e4b9e26c8c9b307831b3ea3ac782b45\n",
      "creating metadata file for aitextgen/08c00c4159e921d4c941ac75732643373aba509d9b352a82bbbb043a94058d98.a552555fdda56a1c7c9a285bccfd44ac8e4b9e26c8c9b307831b3ea3ac782b45\n",
      "https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/merges.txt not found in cache or force_download set to True, downloading to /kaggle/working/aitextgen/tmpfmcb1mr5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5a00e0489cb4db8832f2bd0c472f2f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/merges.txt in cache at aitextgen/12305762709d884a770efe7b0c68a7f4bc918da44e956058d43da0d12f7bea20.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "creating metadata file for aitextgen/12305762709d884a770efe7b0c68a7f4bc918da44e956058d43da0d12f7bea20.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/special_tokens_map.json not found in cache or force_download set to True, downloading to /kaggle/working/aitextgen/tmpjtstvklz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "490a66b6b57043ab9ce3bf4402f7301e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/357 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/special_tokens_map.json in cache at aitextgen/6c3239a63aaf46ec7625b38abfe41fc2ce0b25f90800aefe6526256340d4ab6d.2b8bf81243d08385c806171bc7ced6d2a0dcc7f896ca637f4e777418f7f0cc3c\n",
      "creating metadata file for aitextgen/6c3239a63aaf46ec7625b38abfe41fc2ce0b25f90800aefe6526256340d4ab6d.2b8bf81243d08385c806171bc7ced6d2a0dcc7f896ca637f4e777418f7f0cc3c\n",
      "loading file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/vocab.json from cache at aitextgen/08c00c4159e921d4c941ac75732643373aba509d9b352a82bbbb043a94058d98.a552555fdda56a1c7c9a285bccfd44ac8e4b9e26c8c9b307831b3ea3ac782b45\n",
      "loading file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/merges.txt from cache at aitextgen/12305762709d884a770efe7b0c68a7f4bc918da44e956058d43da0d12f7bea20.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/tokenizer.json from cache at None\n",
      "loading file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/special_tokens_map.json from cache at aitextgen/6c3239a63aaf46ec7625b38abfe41fc2ce0b25f90800aefe6526256340d4ab6d.2b8bf81243d08385c806171bc7ced6d2a0dcc7f896ca637f4e777418f7f0cc3c\n",
      "loading file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/tokenizer_config.json from cache at aitextgen/3cc88b3aa29bb2546db2dc21783292e2a086bb7158c7b5ceddeb24158a85c183.e74f7c3643ee79eb023ead36008be72fe726dada60fa3b2a0569925cfefa1e74\n",
      "https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpx90fh_6f\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d96160be6cd947fd8d48c7395ea406b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/0.98k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/29380fef22a43cbfb3d3a6c8e2f4fd951459584d87c34e4621b30580a54aca84.f0f7ebddfc6e15a23ac33e7fa95cd8cca05edf87cc74f9e3be7905f538a59762\n",
      "creating metadata file for /root/.cache/huggingface/transformers/29380fef22a43cbfb3d3a6c8e2f4fd951459584d87c34e4621b30580a54aca84.f0f7ebddfc6e15a23ac33e7fa95cd8cca05edf87cc74f9e3be7905f538a59762\n"
     ]
    }
   ],
   "source": [
    "# ai = aitextgen(tf_gpt2=\"124M\",  to_gpu=True)\n",
    "ai = aitextgen(model = model,  to_gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-03T15:31:13.851012Z",
     "iopub.status.busy": "2022-10-03T15:31:13.850496Z",
     "iopub.status.idle": "2022-10-03T15:35:26.842446Z",
     "shell.execute_reply": "2022-10-03T15:35:26.838204Z",
     "shell.execute_reply.started": "2022-10-03T15:31:13.850974Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccbd707ec023464494e38aae1b25333d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/896 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:448: LightningDeprecationWarning: Setting `Trainer(gpus=-1)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=-1)` instead.\n",
      "  f\"Setting `Trainer(gpus={gpus!r})` is deprecated in v1.7 and will be removed\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/configuration_validator.py:259: LightningDeprecationWarning: The `Callback.on_batch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_end` instead.\n",
      "  f\"The `Callback.{hook}` hook was deprecated in v1.6 and\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb1f145d2d5849c083c302455418c9a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[1m100 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      "  cinematic still from movie still than his face, in his heart, outrun, vaporware, digital painting, artstation, concept art, smooth, sharp focus, illustration, art alphonse mucha, lois van baarle, ilya kuvshinov, rossdraws, octane render, unreal 5, volumetric, 8 k, unreal 5 mm lens, unreal lens flare, 8 k, cinematic filmes, trending on artstation, unreal 5\"\n",
      "\n",
      "==========\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[1m200 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      " and jean claude meziere and moebius and will eugene de blaas.\n",
      "\n",
      "==========\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[1m300 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      " a portrait of a cybernetic mistress of the dark, cyberpunk concept art by pete mohrbacher and wlop and artgerm and josan gonzales, digital, highly detailed, intricate, sci-fi, synthwave, synthwave, synthwave, synthwave, synthwave, synthwave, synthwave, synthwave, synthwave, 8k, highly detailed, intricate, sci-fi, synthwave, 8k, highly detailed, trending on artstation, deviantart, by pete mohrbacher and gustav mckernan and william - adolphe bouguereau\"\n",
      "\n",
      "==========\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[1m400 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      "a, character concept, portrait, matte, sharp focus, hd, character design, by Greg Rutkowski, talking to alphonse mucha, artgerm, concept art, 4 k, hyperrealism, hyperdetailed\"\n",
      "\n",
      "==========\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[1m500 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m500 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      " of a woman with long dark hair and red eyes, wearing a red dress, holding a bouquet of flowing flowers, drenched body, wet dripping hair, emerging from the water, fantasy, regal, fractal crystal, fractal gems, by stanley artgerm lau, greg rutkowski, alphonse mucha, loish, norman rockwell\"\n",
      "\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "ai.train('input_text_cleaned.txt',\n",
    "         line_by_line=True,\n",
    "         from_cache=False,\n",
    "         num_steps=500,\n",
    "         generate_every=100,\n",
    "         save_every=500,\n",
    "         save_gdrive=False,\n",
    "         learning_rate=1e-3,\n",
    "         fp16=False,\n",
    "         batch_size=1, \n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-03T15:35:35.569173Z",
     "iopub.status.busy": "2022-10-03T15:35:35.568711Z",
     "iopub.status.idle": "2022-10-03T15:35:37.879800Z",
     "shell.execute_reply": "2022-10-03T15:35:37.877931Z",
     "shell.execute_reply.started": "2022-10-03T15:35:35.569135Z"
    }
   },
   "outputs": [],
   "source": [
    "ai.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-03T15:35:44.080685Z",
     "iopub.status.busy": "2022-10-03T15:35:44.080133Z",
     "iopub.status.idle": "2022-10-03T15:35:47.958696Z",
     "shell.execute_reply": "2022-10-03T15:35:47.957302Z",
     "shell.execute_reply.started": "2022-10-03T15:35:44.080633Z"
    }
   },
   "outputs": [],
   "source": [
    "prompt_ai = aitextgen(model_folder = '.', to_gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-03T16:00:38.494155Z",
     "iopub.status.busy": "2022-10-03T16:00:38.493669Z",
     "iopub.status.idle": "2022-10-03T16:00:41.154110Z",
     "shell.execute_reply": "2022-10-03T16:00:41.152537Z",
     "shell.execute_reply.started": "2022-10-03T16:00:38.494123Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mastronaut \u001b[0mernst haeckel, donato giancola, boris vallejo, drew struzan realistic detailed oil painting, urban horror, frank frazetta, saturated colors. realistic shaded lighting, sharp, depth of field, trending on art station, a night, lit by stars, detailed and intricate environment, Tiled wall, by Matteo Pasqualin Volegov, Greg Rutkowski, Greg Rutkowski, Pete Morbacher, Tuomas Korpi, tekkon kinreet, volumetric, octane render, ray tracing, donato giancola, tekkon kinreet, volumetric, octane render, dramatic lighting,. Peter mohrbacher. Mayan by Takato Yamamoto. David Hockney.\"\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(prompt_ai.generate(prompt = \"astronaut \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-03T16:00:47.789865Z",
     "iopub.status.busy": "2022-10-03T16:00:47.788427Z",
     "iopub.status.idle": "2022-10-03T16:00:49.162567Z",
     "shell.execute_reply": "2022-10-03T16:00:49.160959Z",
     "shell.execute_reply.started": "2022-10-03T16:00:47.789804Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "astronaut ernst haeckel, bernich, walter everett, lost edges, by yoichi hatakenaka, masashi kojima, josan gonzales and dan mumford, ayami kojima, takato yamamoto, barclay shaw 8 k, sharp focus, vibrant, intricate, octane render, ray tracing, hphstone, art by Takato Yamamoto and Simon Stalenhag\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt_ai.generate_one(prompt = \"astronaut \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-03T15:37:02.386919Z",
     "iopub.status.busy": "2022-10-03T15:37:02.386457Z",
     "iopub.status.idle": "2022-10-03T15:37:03.866577Z",
     "shell.execute_reply": "2022-10-03T15:37:03.865197Z",
     "shell.execute_reply.started": "2022-10-03T15:37:02.386868Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in sd-prompt-generator-gpt-neo/tokenizer_config.json\n",
      "Special tokens file saved in sd-prompt-generator-gpt-neo/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "ai.save_for_upload('sd-prompt-generator-gpt-neo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-03T15:47:40.027734Z",
     "iopub.status.busy": "2022-10-03T15:47:40.027298Z",
     "iopub.status.idle": "2022-10-03T15:47:40.082735Z",
     "shell.execute_reply": "2022-10-03T15:47:40.081369Z",
     "shell.execute_reply.started": "2022-10-03T15:47:40.027698Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "384123fcc46e4e6ba6ea8464f5283e05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-03T15:52:33.454617Z",
     "iopub.status.busy": "2022-10-03T15:52:33.454148Z",
     "iopub.status.idle": "2022-10-03T15:52:41.563701Z",
     "shell.execute_reply": "2022-10-03T15:52:41.562018Z",
     "shell.execute_reply.started": "2022-10-03T15:52:33.454580Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "The following NEW packages will be installed:\n",
      "  git-lfs\n",
      "0 upgraded, 1 newly installed, 0 to remove and 65 not upgraded.\n",
      "Need to get 3316 kB of archives.\n",
      "After this operation, 11.1 MB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu focal/universe amd64 git-lfs amd64 2.9.2-1 [3316 kB]\n",
      "Fetched 3316 kB in 0s (14.5 MB/s)\n",
      "Selecting previously unselected package git-lfs.\n",
      "(Reading database ... 108827 files and directories currently installed.)\n",
      "Preparing to unpack .../git-lfs_2.9.2-1_amd64.deb ...\n",
      "Unpacking git-lfs (2.9.2-1) ...\n",
      "Setting up git-lfs (2.9.2-1) ...\n",
      "Processing triggers for man-db (2.9.1-1) ...\n"
     ]
    }
   ],
   "source": [
    "!sudo apt-get install git-lfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-03T15:53:42.810264Z",
     "iopub.status.busy": "2022-10-03T15:53:42.809758Z",
     "iopub.status.idle": "2022-10-03T15:53:58.049434Z",
     "shell.execute_reply": "2022-10-03T15:53:58.047738Z",
     "shell.execute_reply.started": "2022-10-03T15:53:42.810230Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cached-path 1.1.5 requires huggingface-hub<0.9.0,>=0.8.1, but you have huggingface-hub 0.10.0 which is incompatible.\n",
      "allennlp 2.10.0 requires protobuf==3.20.0, but you have protobuf 3.19.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip install --upgrade huggingface_hub -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uploading the model to Hugging Face Model Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-03T15:56:34.472635Z",
     "iopub.status.busy": "2022-10-03T15:56:34.472190Z",
     "iopub.status.idle": "2022-10-03T15:56:34.478842Z",
     "shell.execute_reply": "2022-10-03T15:56:34.477342Z",
     "shell.execute_reply.started": "2022-10-03T15:56:34.472604Z"
    }
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-03T15:56:44.681678Z",
     "iopub.status.busy": "2022-10-03T15:56:44.681183Z",
     "iopub.status.idle": "2022-10-03T15:56:44.688019Z",
     "shell.execute_reply": "2022-10-03T15:56:44.686324Z",
     "shell.execute_reply.started": "2022-10-03T15:56:44.681630Z"
    }
   },
   "outputs": [],
   "source": [
    "api = HfApi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-03T15:58:35.427673Z",
     "iopub.status.busy": "2022-10-03T15:58:35.427139Z",
     "iopub.status.idle": "2022-10-03T15:58:59.693671Z",
     "shell.execute_reply": "2022-10-03T15:58:59.692126Z",
     "shell.execute_reply.started": "2022-10-03T15:58:35.427632Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/Amrrs/sd-prompt-generator-gpt-neo/tree/main/.'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "api.upload_folder(\n",
    "    folder_path=\"./sd-prompt-generator-gpt-neo\",\n",
    "    path_in_repo = \".\",\n",
    "    repo_id=\"Amrrs/sd-prompt-generator-gpt-neo\",\n",
    "    repo_type=\"model\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
