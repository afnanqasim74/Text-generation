{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "a175cef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "c8cb5ecc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'artist beyonce marry\" which would see the marriage between Bibi and the goddess, or be called \"beyonce, wife\". There are'}]"
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "generator = pipeline('text-generation', model = 'gpt2')\n",
    "generator(\"artist beyonce marry\", max_length = 30, num_return_sequences=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2aba52b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "617d55b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to t5-base and revision 686f1db (https://huggingface.co/t5-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'the answer to life, the universe and everything'}]"
      ]
     },
     "execution_count": 428,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "text2text_generator = pipeline(\"text2text-generation\")\n",
    "text2text_generator(\"question: What is 42 ? context: 42 is the answer to life, the universe and everything\")\n",
    "[{'generated_text': 'the answer to life, the universe and everything'}]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "6b94dfe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi how  are \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next word predictions:\n",
      "1: so  blessed  you  with the  help  of  this\n",
      "2: going  to  do  to  do  it  on  my\n",
      "3: lots of people  a lot  of us  are  a lot  a lot\n",
      "Select the next word (enter the corresponding number): 1\n",
      "Selected word: so  blessed  you  with the  help  of  this\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">11</span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 8 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 9 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">while</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>:                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">10 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Take user input for the next word</span>                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>11 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>user_input = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>(prompt_text)                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">12 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">13 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Append the user input to the prompt text</span>                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">14 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>prompt_text += user_input + <span style=\"color: #808000; text-decoration-color: #808000\">\" \"</span>                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">D:\\run\\anaconda\\lib\\site-packages\\ipykernel\\kernelbase.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1175</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">raw_input</span>                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1172 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> StdinNotImplementedError(                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1173 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"raw_input was called, but this frontend does not support input requests</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1174 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>)                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1175 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._input_request(                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1176 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">str</span>(prompt),                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1177 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._parent_ident[<span style=\"color: #808000; text-decoration-color: #808000\">\"shell\"</span>],                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1178 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.get_parent(<span style=\"color: #808000; text-decoration-color: #808000\">\"shell\"</span>),                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">D:\\run\\anaconda\\lib\\site-packages\\ipykernel\\kernelbase.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1217</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_input_request</span>                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1214 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">break</span>                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1215 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">except</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">KeyboardInterrupt</span>:                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1216 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># re-raise KeyboardInterrupt, to truncate traceback</span>                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1217 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">KeyboardInterrupt</span>(<span style=\"color: #808000; text-decoration-color: #808000\">\"Interrupted by user\"</span>) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">from</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1218 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">except</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">Exception</span>:                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1219 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.log.warning(<span style=\"color: #808000; text-decoration-color: #808000\">\"Invalid Message:\"</span>, exc_info=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>)                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1220 </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">KeyboardInterrupt: </span>Interrupted by user\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m11\u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 8 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 9 \u001b[0m\u001b[94mwhile\u001b[0m \u001b[94mTrue\u001b[0m:                                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m10 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# Take user input for the next word\u001b[0m                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m11 \u001b[2m│   \u001b[0muser_input = \u001b[96minput\u001b[0m(prompt_text)                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m12 \u001b[0m\u001b[2m│   \u001b[0m                                                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m13 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# Append the user input to the prompt text\u001b[0m                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m14 \u001b[0m\u001b[2m│   \u001b[0mprompt_text += user_input + \u001b[33m\"\u001b[0m\u001b[33m \u001b[0m\u001b[33m\"\u001b[0m                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mD:\\run\\anaconda\\lib\\site-packages\\ipykernel\\kernelbase.py\u001b[0m:\u001b[94m1175\u001b[0m in \u001b[92mraw_input\u001b[0m                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1172 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mraise\u001b[0m StdinNotImplementedError(                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1173 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[33m\"\u001b[0m\u001b[33mraw_input was called, but this frontend does not support input requests\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1174 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1175 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m._input_request(                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1176 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mstr\u001b[0m(prompt),                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1177 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m._parent_ident[\u001b[33m\"\u001b[0m\u001b[33mshell\u001b[0m\u001b[33m\"\u001b[0m],                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1178 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m.get_parent(\u001b[33m\"\u001b[0m\u001b[33mshell\u001b[0m\u001b[33m\"\u001b[0m),                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mD:\\run\\anaconda\\lib\\site-packages\\ipykernel\\kernelbase.py\u001b[0m:\u001b[94m1217\u001b[0m in \u001b[92m_input_request\u001b[0m                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1214 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0m\u001b[94mbreak\u001b[0m                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1215 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mexcept\u001b[0m \u001b[96mKeyboardInterrupt\u001b[0m:                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1216 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[2m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1217 \u001b[2m│   │   │   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mKeyboardInterrupt\u001b[0m(\u001b[33m\"\u001b[0m\u001b[33mInterrupted by user\u001b[0m\u001b[33m\"\u001b[0m) \u001b[94mfrom\u001b[0m \u001b[94mNone\u001b[0m                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1218 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mexcept\u001b[0m \u001b[96mException\u001b[0m:                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1219 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[96mself\u001b[0m.log.warning(\u001b[33m\"\u001b[0m\u001b[33mInvalid Message:\u001b[0m\u001b[33m\"\u001b[0m, exc_info=\u001b[94mTrue\u001b[0m)                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1220 \u001b[0m                                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mKeyboardInterrupt: \u001b[0mInterrupted by user\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the language model for next word prediction\n",
    "next_word_predictor = pipeline(\"text-generation\", model=\"gpt2\", max_length=30, num_return_sequences=3)\n",
    "\n",
    "# Prompt text for generating next word predictions\n",
    "prompt_text = \"\"\n",
    "\n",
    "while True:\n",
    "    # Take user input for the next word\n",
    "    user_input = input(prompt_text)\n",
    "\n",
    "    # Append the user input to the prompt text\n",
    "    prompt_text += user_input + \" \"\n",
    "\n",
    "    # Generate next word predictions\n",
    "    predictions = next_word_predictor(prompt_text, do_sample=True, temperature=0.8)\n",
    "\n",
    "    # Extract the predicted next words\n",
    "    next_words = [prediction['generated_text'].replace(prompt_text, '').strip() for prediction in predictions]\n",
    "\n",
    "    # Print the next word predictions\n",
    "    print(\"Next word predictions:\")\n",
    "    for i, word in enumerate(next_words):\n",
    "        print(f\"{i + 1}: {word}\")\n",
    "\n",
    "    # Break the loop if the user enters an empty string or presses Enter\n",
    "    if user_input == \"\":\n",
    "        break\n",
    "\n",
    "    # Update the prompt text with the selected word\n",
    "    selected_word_index = int(input(\"Select the next word (enter the corresponding number): \")) - 1\n",
    "    selected_word = next_words[selected_word_index]\n",
    "    prompt_text += selected_word + \" \"\n",
    "    print(f\"Selected word: {selected_word}\")\n",
    "\n",
    "print(\"Final prompt text:\", prompt_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d085f7fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jamal is a eam lead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next word predictions:\n",
      "1: singer and has worked for the British rock band Death Cab for Cutie.\n",
      "\n",
      "\"I'm thrilled to have\n",
      "2: on your website. I've been using it for almost 30 years and am an expert in it. I have noticed\n",
      "3: player who has started only one game in all MLS regular season matches, and only joined the team for the second half\n",
      "jamal is a eam lead1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next word predictions:\n",
      "1: . The first half of the match had been very impressive due to his skill and the fact he finished with an\n",
      "2: .\n",
      "\n",
      "\"I have never been more excited to be a part of The Black Sheep,\" said the 40\n",
      "3: s it's been a while since Henrietta. It's been nearly four years since he first left school.\n",
      "jamal is a eam lead1ls fjsfljk ' 'k ja dj dj a\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next word predictions:\n",
      "1: l r o l y u w n t\n",
      "2: 'dj'''b b d a\n",
      "3: lt lg lm le m3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in callback BaseSelectorEventLoop._read_from_self()\n",
      "handle: <Handle BaseSelectorEventLoop._read_from_self()>\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\run\\anaconda\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"D:\\run\\anaconda\\lib\\asyncio\\selector_events.py\", line 115, in _read_from_self\n",
      "    data = self._ssock.recv(4096)\n",
      "ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host\n",
      "Exception in callback BaseSelectorEventLoop._read_from_self()\n",
      "handle: <Handle BaseSelectorEventLoop._read_from_self()>\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\run\\anaconda\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"D:\\run\\anaconda\\lib\\asyncio\\selector_events.py\", line 115, in _read_from_self\n",
      "    data = self._ssock.recv(4096)\n",
      "ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host\n",
      "Exception in callback BaseSelectorEventLoop._read_from_self()\n",
      "handle: <Handle BaseSelectorEventLoop._read_from_self()>\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\run\\anaconda\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"D:\\run\\anaconda\\lib\\asyncio\\selector_events.py\", line 115, in _read_from_self\n",
      "    data = self._ssock.recv(4096)\n",
      "ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host\n",
      "Exception in callback BaseSelectorEventLoop._read_from_self()\n",
      "handle: <Handle BaseSelectorEventLoop._read_from_self()>\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\run\\anaconda\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"D:\\run\\anaconda\\lib\\asyncio\\selector_events.py\", line 115, in _read_from_self\n",
      "    data = self._ssock.recv(4096)\n",
      "ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host\n",
      "Exception in callback BaseSelectorEventLoop._read_from_self()\n",
      "handle: <Handle BaseSelectorEventLoop._read_from_self()>\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\run\\anaconda\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"D:\\run\\anaconda\\lib\\asyncio\\selector_events.py\", line 115, in _read_from_self\n",
      "    data = self._ssock.recv(4096)\n",
      "ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host\n",
      "Exception in callback BaseSelectorEventLoop._read_from_self()\n",
      "handle: <Handle BaseSelectorEventLoop._read_from_self()>\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\run\\anaconda\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"D:\\run\\anaconda\\lib\\asyncio\\selector_events.py\", line 115, in _read_from_self\n",
      "    data = self._ssock.recv(4096)\n",
      "ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host\n",
      "Exception in callback BaseSelectorEventLoop._read_from_self()\n",
      "handle: <Handle BaseSelectorEventLoop._read_from_self()>\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\run\\anaconda\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"D:\\run\\anaconda\\lib\\asyncio\\selector_events.py\", line 115, in _read_from_self\n",
      "    data = self._ssock.recv(4096)\n",
      "ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host\n",
      "Exception in callback BaseSelectorEventLoop._read_from_self()\n",
      "handle: <Handle BaseSelectorEventLoop._read_from_self()>\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\run\\anaconda\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"D:\\run\\anaconda\\lib\\asyncio\\selector_events.py\", line 115, in _read_from_self\n",
      "    data = self._ssock.recv(4096)\n",
      "ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host\n",
      "Exception in callback BaseSelectorEventLoop._read_from_self()\n",
      "handle: <Handle BaseSelectorEventLoop._read_from_self()>\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\run\\anaconda\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"D:\\run\\anaconda\\lib\\asyncio\\selector_events.py\", line 115, in _read_from_self\n",
      "    data = self._ssock.recv(4096)\n",
      "ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host\n",
      "Exception in callback BaseSelectorEventLoop._read_from_self()\n",
      "handle: <Handle BaseSelectorEventLoop._read_from_self()>\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\run\\anaconda\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"D:\\run\\anaconda\\lib\\asyncio\\selector_events.py\", line 115, in _read_from_self\n",
      "    data = self._ssock.recv(4096)\n",
      "ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host\n",
      "Exception in callback BaseSelectorEventLoop._read_from_self()\n",
      "handle: <Handle BaseSelectorEventLoop._read_from_self()>\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\run\\anaconda\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"D:\\run\\anaconda\\lib\\asyncio\\selector_events.py\", line 115, in _read_from_self\n",
      "    data = self._ssock.recv(4096)\n",
      "ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host\n",
      "Exception in callback BaseSelectorEventLoop._read_from_self()\n",
      "handle: <Handle BaseSelectorEventLoop._read_from_self()>\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\run\\anaconda\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"D:\\run\\anaconda\\lib\\asyncio\\selector_events.py\", line 115, in _read_from_self\n",
      "    data = self._ssock.recv(4096)\n",
      "ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host\n",
      "Exception in callback BaseSelectorEventLoop._read_from_self()\n",
      "handle: <Handle BaseSelectorEventLoop._read_from_self()>\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\run\\anaconda\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"D:\\run\\anaconda\\lib\\asyncio\\selector_events.py\", line 115, in _read_from_self\n",
      "    data = self._ssock.recv(4096)\n",
      "ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host\n",
      "Exception in callback BaseSelectorEventLoop._read_from_self()\n",
      "handle: <Handle BaseSelectorEventLoop._read_from_self()>\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\run\\anaconda\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"D:\\run\\anaconda\\lib\\asyncio\\selector_events.py\", line 115, in _read_from_self\n",
      "    data = self._ssock.recv(4096)\n",
      "ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host\n",
      "Exception in callback BaseSelectorEventLoop._read_from_self()\n",
      "handle: <Handle BaseSelectorEventLoop._read_from_self()>\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\run\\anaconda\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"D:\\run\\anaconda\\lib\\asyncio\\selector_events.py\", line 115, in _read_from_self\n",
      "    data = self._ssock.recv(4096)\n",
      "ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host\n",
      "Exception in callback BaseSelectorEventLoop._read_from_self()\n",
      "handle: <Handle BaseSelectorEventLoop._read_from_self()>\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\run\\anaconda\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"D:\\run\\anaconda\\lib\\asyncio\\selector_events.py\", line 115, in _read_from_self\n",
      "    data = self._ssock.recv(4096)\n",
      "ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host\n",
      "Exception in callback BaseSelectorEventLoop._read_from_self()\n",
      "handle: <Handle BaseSelectorEventLoop._read_from_self()>\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\run\\anaconda\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"D:\\run\\anaconda\\lib\\asyncio\\selector_events.py\", line 115, in _read_from_self\n",
      "    data = self._ssock.recv(4096)\n",
      "ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in callback BaseSelectorEventLoop._read_from_self()\n",
      "handle: <Handle BaseSelectorEventLoop._read_from_self()>\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\run\\anaconda\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"D:\\run\\anaconda\\lib\\asyncio\\selector_events.py\", line 115, in _read_from_self\n",
      "    data = self._ssock.recv(4096)\n",
      "ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host\n",
      "Exception in callback BaseSelectorEventLoop._read_from_self()\n",
      "handle: <Handle BaseSelectorEventLoop._read_from_self()>\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\run\\anaconda\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"D:\\run\\anaconda\\lib\\asyncio\\selector_events.py\", line 115, in _read_from_self\n",
      "    data = self._ssock.recv(4096)\n",
      "ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jamal is a eam lead1ls fjsfljk ' 'k ja dj dj a s;lfgk sdfg\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the language model for next word prediction\n",
    "next_word_predictor = pipeline(\"text-generation\", model=\"gpt2\", max_length=30, num_return_sequences=3)\n",
    "\n",
    "# Prompt text for generating next word predictions\n",
    "prompt_text = \"\"\n",
    "\n",
    "while True:\n",
    "    # Take user input for sentence continuation\n",
    "    user_input = input(prompt_text)\n",
    "\n",
    "    # Append the user input to the prompt text\n",
    "    prompt_text += user_input\n",
    "\n",
    "    # Generate next word predictions\n",
    "    predictions = next_word_predictor(prompt_text, do_sample=True, temperature=0.8)\n",
    "\n",
    "    # Extract the next word predictions\n",
    "    next_words = [prediction['generated_text'].replace(prompt_text, '').strip() for prediction in predictions]\n",
    "\n",
    "    # Print the next word predictions\n",
    "    print(\"Next word predictions:\")\n",
    "    for i, word in enumerate(next_words):\n",
    "        print(f\"{i + 1}: {word}\")\n",
    "\n",
    "    # Break the loop if the user enters an empty string or presses Enter\n",
    "    if user_input == \"\":\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfc473e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import GPT2Config, GPT2Tokenizer, GPT2LMHeadModel, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(\"final_review_1.csv\")  # Replace \"your_dataset.csv\" with your actual CSV file path\n",
    "df = df.head(100)\n",
    "questions = df[\"question\"].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5177baf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tokenize the questions\n",
    "tokenized_questions = [tokenizer.encode(question) for question in questions]\n",
    "\n",
    "# Flatten the list of tokenized questions\n",
    "input_tokens = [token for question in tokenized_questions for token in question]\n",
    "\n",
    "# Save the tokenized questions to a file\n",
    "with open(\"tokenized_questions.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    for token in input_tokens:\n",
    "        file.write(str(token) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9604124",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\run\\anaconda\\lib\\site-packages\\transformers\\data\\datasets\\language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n",
      "D:\\run\\anaconda\\lib\\site-packages\\torch\\cuda\\__init__.py:107: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ..\\c10\\cuda\\CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "D:\\run\\anaconda\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mafnanqasim74\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a336c434ea646deaca6c92dcb9bc019",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\Afnan Qasim\\Desktop\\text generation\\wandb\\run-20230706_172951-tf8kkfwa</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/afnanqasim74/huggingface/runs/tf8kkfwa' target=\"_blank\">effortless-pine-3</a></strong> to <a href='https://wandb.ai/afnanqasim74/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/afnanqasim74/huggingface' target=\"_blank\">https://wandb.ai/afnanqasim74/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/afnanqasim74/huggingface/runs/tf8kkfwa' target=\"_blank\">https://wandb.ai/afnanqasim74/huggingface/runs/tf8kkfwa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='37' max='13810' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   37/13810 06:40 < 43:46:51, 0.09 it/s, Epoch 0.03/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the TextDataset\n",
    "dataset = TextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"tokenized_questions.txt\",\n",
    "    block_size=128  # Set the desired block size\n",
    ")\n",
    "\n",
    "# Configure the language model\n",
    "config = GPT2Config(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    n_positions=512,  # Set the maximum sequence length\n",
    "    n_ctx=512\n",
    ")\n",
    "model = GPT2LMHeadModel(config)\n",
    "\n",
    "# Define the data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False  # Set to True if you want to perform masked language modeling\n",
    ")\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"output_dir\",  # Set the output directory for saving the trained model\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=10,  # Set the number of training epochs\n",
    "    per_device_train_batch_size=8,  # Set the batch size for training\n",
    "    save_total_limit=2,\n",
    "    save_steps=500,  # Set the number of steps between saving checkpoints\n",
    "    logging_steps=100,  # Set the number of steps between logging\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,  # Set the number of steps between evaluation\n",
    "    learning_rate=5e-5  # Set the learning rate\n",
    ")\n",
    "\n",
    "# Create the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2c7dd84",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Save the trained model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241m.\u001b[39msave_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msaved_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "# Save the trained model\n",
    "trainer.save_model(\"saved_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2bad6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6988687",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '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' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[0;32m      3\u001b[0m API_URL \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://api-inference.huggingface.co/models/bigscience/bloom-560m\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 4\u001b[0m headers \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAuthorization\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBearer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhf_oZYOJqdhnRpjncynCXnNurENdYhAyKPufd\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mquery\u001b[39m(payload):\n\u001b[0;32m      7\u001b[0m \tresponse \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mpost(API_URL, headers\u001b[38;5;241m=\u001b[39mheaders, json\u001b[38;5;241m=\u001b[39mpayload)\n",
      "\u001b[1;31mNameError\u001b[0m: name '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' is not defined"
     ]
    }
   ],
   "source": [
    "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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87a29258",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "API_TOKEN = \"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\"\n",
    "API_URL = \"https://api-inference.huggingface.co/models/bigscience/bloom-560m\"\n",
    "headers = {\"Authorization\": f\"Bearer {API_TOKEN}\"}\n",
    "\n",
    "\n",
    "def query(payload):\n",
    "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
    "\treturn response.json()\n",
    "\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f029d1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = query({\n",
    "\t\"inputs\": \"Can you please let us know more details about your \",\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "id": "714bf603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'error': 'Model bigscience/bloom-560m is currently loading',\n",
       " 'estimated_time': 44.74127197265625}"
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8dafdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
