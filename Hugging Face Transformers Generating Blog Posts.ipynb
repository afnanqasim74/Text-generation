{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://hackernoon.com/no-code-needs-to-adapt-to-specialists-an-argument-uo1w328p    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Install and Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in d:\\run\\anaconda\\lib\\site-packages (4.30.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in d:\\run\\anaconda\\lib\\site-packages (from transformers) (0.15.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in d:\\run\\anaconda\\lib\\site-packages (from transformers) (0.11.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\run\\anaconda\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\run\\anaconda\\lib\\site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in d:\\run\\anaconda\\lib\\site-packages (from transformers) (0.3.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in d:\\run\\anaconda\\lib\\site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: requests in d:\\run\\anaconda\\lib\\site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\run\\anaconda\\lib\\site-packages (from transformers) (22.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\run\\anaconda\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: filelock in d:\\run\\anaconda\\lib\\site-packages (from transformers) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\run\\anaconda\\lib\\site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
      "Requirement already satisfied: fsspec in d:\\run\\anaconda\\lib\\site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
      "Requirement already satisfied: colorama in d:\\run\\anaconda\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\run\\anaconda\\lib\\site-packages (from requests->transformers) (2023.5.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in d:\\run\\anaconda\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\run\\anaconda\\lib\\site-packages (from requests->transformers) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\run\\anaconda\\lib\\site-packages (from requests->transformers) (3.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -orch (d:\\run\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (d:\\run\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (d:\\run\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (d:\\run\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (d:\\run\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (d:\\run\\anaconda\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2-large\", pad_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load the pretrained model and tokenizer\n",
    "model_name = \"gpt2\"  # Example model name, replace with your desired model\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Save the model and tokenizer as pickles\n",
    "model_path = \"model.pkl\"\n",
    "tokenizer_path = \"tokenizer.pkl\"\n",
    "\n",
    "with open(model_path, \"wb\") as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "with open(tokenizer_path, \"wb\") as f:\n",
    "    pickle.dump(tokenizer, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'circket match is the best thing to perform'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer.encode(sentence, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  66,  343,  694,  316, 2872,  318,  262, 1266, 1517,  284, 1620]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate text until the output length (which includes the context length) reaches 50\n",
    "output = model.generate(input_ids, max_length=50, num_beams=5, no_repeat_ngram_size=2, early_stopping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  66,  343,  694,  316, 2872,  318,  262, 1266, 1517,  284, 1620,  287,\n",
       "          262,  995,   13,  198,  198,    1,   40,  892,  340,  338,  257, 1049,\n",
       "         3663,  329,  502,  284,  905,  644,  314,  460,  466,   13,  314, 1101,\n",
       "         1107, 2045, 2651,  284,  340,   13,  632,  338, 1016,  284,  307,  257,\n",
       "         1256,  286]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "circket match is the best thing to perform in the world.\n",
      "\n",
      "\"I think it's a great opportunity for me to show what I can do. I'm really looking forward to it. It's going to be a lot of\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load the saved model and tokenizer from pickles\n",
    "model_path = \"model.pkl\"\n",
    "tokenizer_path = \"tokenizer.pkl\"\n",
    "\n",
    "with open(model_path, \"rb\") as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "with open(tokenizer_path, \"rb\") as f:\n",
    "    tokenizer = pickle.load(f)\n",
    "\n",
    "# Set pad token ID explicitly\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Story:\n",
      "pakistan, Afghanistan, Pakistan, Saudi Arabia, Sudan, Syria, Yemen, Somalia, and Yemen.\n",
      "\n",
      "The U.S.-led coalition has carried out more than 1,200 airstrikes in Afghanistan since the start of the year, according to the United Nations. That's up from just over 2,000 in 2014, when the coalition launched its first air campaign against the Taliban in the country.\n"
     ]
    }
   ],
   "source": [
    "# Define input text\n",
    "input_text = \"pakistan\"\n",
    "\n",
    "# Tokenize input text\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Ensure attention mask and pad token ID are set\n",
    "input_ids = input_ids.to(model.device)\n",
    "attention_mask = input_ids.ne(tokenizer.pad_token_id).float()\n",
    "\n",
    "# Generate output\n",
    "output = model.generate(input_ids, attention_mask=attention_mask,num_beams=5, no_repeat_ngram_size=2, early_stopping=True, max_length=100)\n",
    "\n",
    "# Decode output tokens to text\n",
    "generated_story = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Print generated story\n",
    "print(\"Generated Story:\")\n",
    "print(generated_story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "input_text = \"i will be died soon\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Tokenize input text\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Ensure attention mask and pad token ID are set\n",
    "input_ids = input_ids.to(model.device)\n",
    "attention_mask = input_ids.ne(tokenizer.pad_token_id).float()\n",
    "\n",
    "# Generate output\n",
    "output = model.generate(input_ids, attention_mask=attention_mask,\n",
    "                        num_beams=100, no_repeat_ngram_size=1,\n",
    "                        early_stopping=True, max_length=100)\n",
    "\n",
    "# Decode output tokens to text\n",
    "generated_story = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Print generated story word by word\n",
    "print(\"Generated Story:\")\n",
    "for word in generated_story.split():\n",
    "    print(word, end=\" \")\n",
    "    time.sleep(0.1)  # Adjust sleep time if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
